{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1-izC4ZKHJ9"
      },
      "source": [
        "<h1 style=\"text-align: center\">\n",
        "Machine Learning HW1 </br>\n",
        "MLE & MAP in Python\n",
        "</h1>\n",
        "\n",
        "#### Name: Ehasan Hassanbeygi\n",
        "\n",
        "#### Std. Number: 402211723"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhTuYwHYbE_1"
      },
      "source": [
        "## Objective\n",
        "This exercise will help you gain a deeper understanding of, and insights into, Maximum Likelihood Estimation (MLE) and Maximum A Posteriori (MAP) estimation$\\textit{Maximum Likelihood Estimation (MLE) and Maximum A Posteriori (MAP) }$ :) \\\\\n",
        "Letâ€™s say you have a barrel of apples that are all different sizes. You pick an apple at random, and you want to know its weight. Unfortunately, all you have is a broken scale. answer the questions below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSaLb6AYa9DJ"
      },
      "source": [
        "1) For the sake of this section, lets imagine a farmer tells you that the scale returns the weight of the object with an error of +/- a standard deviation of 5g. We can describe this mathematically as:\n",
        "$$\n",
        "measurement = weight + \\mathcal{N}(0, 5g)\n",
        "$$\n",
        "You can weigh the apple as many times as you want, so weigh it 100 times.\n",
        "plot its histogram of your 100 measurements. (y axis is the counts and x-axis is the measured weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hPMnHTcia07a"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "real_w = 10\n",
        "measurements = []\n",
        "for i in range(100):\n",
        "        measurements.append(real_w + np.random.normal(loc=0, scale=5))\n",
        "\n",
        "# alternative way\n",
        "# measurements = np.random.normal(loc=real_w, scale=5, size=100)\n",
        "\n",
        "plt.hist(measurements)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HD9Mqy-bcPi5"
      },
      "source": [
        "2) Find the average weight of the apple.\n",
        "Is it a good guess? state your reason."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xlCBTC0lcPKa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "average weight of measurements is: 10.089641359675968\n"
          ]
        }
      ],
      "source": [
        "print(f'average weight of measurements is: {np.mean(measurements)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (2)\n",
        "We are approximating the real weight by considering a fixed amout of samples. So the quality of our guess heavily depends on the number of samlpes observed so far.\n",
        "If our observed samples are biased or we simply do not have enough of them, then our approximation is going to be poor. But if it meets the requirements of our problem (good amount of independent/unbiased samples) then it can be a good guess."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-P9PJuKcrbq"
      },
      "source": [
        "3) we are going to use grid approximation for calculating the MLE. here is the link if you wnat to get more fimilar with this technique:\n",
        "https://www.bayesrulesbook.com/chapter-6\n",
        "\n",
        "Our end goal is to find the weight of the apple, given the data we have. To formulate it in a Bayesian way: Weâ€™ll ask what is the probability of the apple having weight, $w$, given the measurements we took, $X$. And, because we're formulating this in a Bayesian way, we use Bayesâ€™ Law to find the answer:\n",
        "\n",
        "$$\n",
        "P(w|X) = \\frac{P(X|w)P(w)}{P(X)}\n",
        "$$\n",
        "\n",
        "If we make no assumptions about the initial weight of our apple, then we can drop $P(w)$. Weâ€™ll say all sizes of apples are equally likely (weâ€™ll revisit this assumption in the MAP approximation).\n",
        "\n",
        "Furthermore, weâ€™ll drop $P(X)$ - the probability of seeing our data. This is a normalization constant and will be important if we do want to know the probabilities of apple weights. But, for right now, our end goal is to only to find the most probable weight. $P(X)$ is independent of $w$, so we can drop it if weâ€™re doing relative comparisons.\n",
        "\n",
        "This leaves us with $P(X|w)$, our likelihood, as in, what is the likelihood that we would see the data, $X$, given an apple of weight $w$. If we maximize this, we maximize the probability that we will guess the right weight.\n",
        "\n",
        "The grid approximation is probably the simplest way to do this. Basically, weâ€™ll systematically step through different weight guesses, and compare what it would look like if this hypothetical weight were to generate data. Weâ€™ll compare this hypothetical data to our real data and pick the one that matches the best.\n",
        "\n",
        "To formulate this mathematically:\n",
        "\n",
        "For each of these guesses, weâ€™re asking \"what is the probability that the data we have, came from the distribution that our weight guess would generate\". Because each measurement is independent from another, we can break the above equation down into finding the probability on a per measurement basis:\n",
        "\n",
        "$$\n",
        "P(X|w) = \\prod_{i}^{N} p(x_i|w)\n",
        "$$\n",
        "\n",
        "So, if we multiply the probability that we would see each individual data point - given our weight guess - then we can find one number comparing our weight guess to all of our data.\n",
        "\n",
        "The peak in the likelihood is the weight of the apple.\n",
        "\n",
        "To make it computationally easier,\n",
        "\n",
        "$$\n",
        "\\log P(X|w) = \\log \\prod_{i}^{N} p(x_i|w) = \\sum_{i}^{N} \\log p(d_i|w)\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "a) Why did we use log likelihood? Is it ok to do so?\n",
        "\n",
        "b) do the grid approximation and complete the cell below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (3.a)\n",
        "We have two reasons to use log likelihood instead of likelihood itself\n",
        "- likelihood is $P(X|w) = \\prod_{i=1}^{N} p(x_i|w)$ so if he have a lot of samples then we are multiplying a lot of small values ($\\leq 1$) and eventually they will vanish. so we use logarithm and add the values instead\n",
        "\n",
        "- also if we want to calculate $\\hat{w}$ by  solving $\\frac{\\partial likelihood}{\\partial w} = 0$, our calculations may be hard. so based on the distribution of $P(X|w)$ getting the logarithm may help us by making our calculations easier\n",
        "\n",
        "\n",
        "Doing so is allowed because the logarithm is a monotonically increasing function.This means it won't change the peaks of the original function. Whatever $\\hat{w}$ maximizes likelihood will also maximize log likelihood"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9NnWmxzTiRfr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10.204081632653061\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'\\n# Calculate the maximum likelihood estimate of a parameter in a normal distribution.\\n# First calculate the log likelihoods for a range of weight guesses.\\n# For each weight guess, assume that the data comes from a normal distribution with that mean and a standard deviation of 10.\\n# Then calculate the log of the probability density function (pdf) of the data under this assumption.\\n# The sum of these log pdf values is the total log likelihood for that weight guess.\\n# After calculating the log likelihoods for all weight guesses, find the weight guess with the maximum log likelihood.\\n# This is the maximum likelihood estimate of the weight.\\n'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from scipy.stats import norm\n",
        "import numpy as np\n",
        "\n",
        "weight_grid = np.linspace(0, 100)\n",
        "\n",
        "ll_list = np.asarray(\n",
        "    [norm(weight_guess, 10).logpdf(measurements).sum() for weight_guess in weight_grid])\n",
        "print(weight_grid[np.argmax(ll_list)])\n",
        "\n",
        "\"\"\"\n",
        "# Calculate the maximum likelihood estimate of a parameter in a normal distribution.\n",
        "# First calculate the log likelihoods for a range of weight guesses.\n",
        "# For each weight guess, assume that the data comes from a normal distribution with that mean and a standard deviation of 10.\n",
        "# Then calculate the log of the probability density function (pdf) of the data under this assumption.\n",
        "# The sum of these log pdf values is the total log likelihood for that weight guess.\n",
        "# After calculating the log likelihoods for all weight guesses, find the weight guess with the maximum log likelihood.\n",
        "# This is the maximum likelihood estimate of the weight.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN3lt2npcc2S"
      },
      "source": [
        "Play around with the code and try to answer the following questions regarding MLE and MAP. You can draw plots to visualize as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ezcWTpNQamCL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average measurement: 89.511 g\n",
            "Maximum Likelihood estimate: 89.444 g\n",
            "Maximum A Posterior estimate: 51.253 g\n",
            "The true weight of the apple was: 91.187 g\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import norm, invgamma\n",
        "\n",
        "\n",
        "# The barrel of apples\n",
        "# The average apples is between 70-100 g\n",
        "BARREL = np.random.normal(loc=85, scale=20, size=100)\n",
        "# Grid\n",
        "WEIGHT_GUESSES = np.linspace(1, 200, 100)\n",
        "ERROR_GUESSES = np.linspace(.1, 50, 100)\n",
        "\n",
        "# NOTE: Try changing the scale error\n",
        "# in practice, you would not know this number\n",
        "SCALE_ERR = 5\n",
        "\n",
        "# NOTE: Try changing the number of measurements taken\n",
        "N_MEASURMENTS = 10\n",
        "\n",
        "# NOTE: Try changing the prior values and distributions\n",
        "PRIOR_WEIGHT = norm(50, 1).logpdf(WEIGHT_GUESSES)\n",
        "PRIOR_ERR = invgamma(4).logpdf(ERROR_GUESSES)\n",
        "\n",
        "LOG_PRIOR_GRID = np.add.outer(PRIOR_ERR, PRIOR_WEIGHT)\n",
        "\n",
        "\n",
        "def read_scale(apple):\n",
        "    return apple + np.random.normal(loc=0, scale=SCALE_ERR)\n",
        "\n",
        "\n",
        "def get_log_likelihood_grid(measurments):\n",
        "    log_liklelihood = [\n",
        "        [\n",
        "            norm(weight_guess, error_guess).logpdf(measurments).sum()\n",
        "            for weight_guess in WEIGHT_GUESSES\n",
        "        ]\n",
        "        for error_guess in ERROR_GUESSES\n",
        "    ]\n",
        "    return np.asarray(log_liklelihood)\n",
        "\n",
        "def get_mle(measurements):\n",
        "    ll_grid = get_log_likelihood_grid(measurements)\n",
        "    sigma_hat_index, mu_hat_index = np.unravel_index(\n",
        "        np.argmax(ll_grid), ll_grid.shape)\n",
        "    \n",
        "    return WEIGHT_GUESSES[mu_hat_index]\n",
        "\n",
        "    \"\"\"\n",
        "    Calculate the log-likelihood for each measurement in the grid.\n",
        "    Find the index of the maximum log-likelihood in the grid.\n",
        "    Return the weight guess corresponding to the maximum log-likelihood.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "def get_map(measurements):\n",
        "    ll_grid = get_log_likelihood_grid(measurements)\n",
        "    sigma_hat_index, mu_hat_index = np.unravel_index(\n",
        "        np.argmax(ll_grid + LOG_PRIOR_GRID), ll_grid.shape)\n",
        "    \n",
        "    return WEIGHT_GUESSES[mu_hat_index]\n",
        "\n",
        "    \"\"\"\n",
        "    Calculate the log-likelihood for each measurement in the grid.\n",
        "    Add the log prior to the log likelihood to get the log posterior.\n",
        "    Find the index of the maximum log posterior in the grid.\n",
        "    Return the weight guess corresponding to the maximum log posterior.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "# Pick an apple at random\n",
        "apple = np.random.choice(BARREL)\n",
        "\n",
        "# weight the apple\n",
        "measurements = np.asarray([read_scale(apple) for _ in range(N_MEASURMENTS)])\n",
        "\n",
        "print(f\"Average measurement: {measurements.mean():.3f} g\")\n",
        "print(f\"Maximum Likelihood estimate: {get_mle(measurements):.3f} g\")\n",
        "print(f\"Maximum A Posterior estimate: {get_map(measurements):.3f} g\")\n",
        "print(f\"The true weight of the apple was: {apple:.3f} g\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI_541TpetKk"
      },
      "source": [
        "<h3><i><i> Questions</h3>\n",
        "1.\n",
        "How sensitive is the MAP measurement to the choice of prior?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (1)\n",
        "It's so sensetive to the choice of prior distribution. forexample in your code, you initialized the prior to have \n",
        "$$\\mu=50 , \\sigma = 1$$\n",
        "which means that we have prior knowledge that the weights are around 50g and we are also sure of it (because the standard deviation is so small).\n",
        "So MAP estimation behaves poorly due to inaccurate prior data and will return something around 50g.\n",
        "But if we set the prior knowledge based on the real data (distribution of the barrel) it will behave so much better. Forexample just by increasing the standard deviation from 1 to 20, the results of MAP estimation almost became the same as MLE.\n",
        "\n",
        "In the general case MAP estimation has the potential to be better than MLE, becuase it doesn't only rely on the observations and includes some domain knowledge in its estimation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMV-wgYXes_O"
      },
      "source": [
        "<h3><i><i></h3>\n",
        "2. How sensitive is the MLE and MAP answer to the grid size?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (2)\n",
        "\n",
        "In general the space for $\\theta$ is $R^2$ but we are sampling this space and only testing limited combinations of ($\\mu , \\sigma$).\n",
        "Imagine that the theoretical $\\hat{\\theta}$ is somewhere in that space, our method is just picking the $\\theta$ in our grid that is the closest to the theoretical $\\hat{\\theta}$.\n",
        "\n",
        "If we increase the size of our grid, we are going to have closer points to the real estimations to pick from. So MLE and MAP are sensitive to the grid size in the way that if we increse it, we will have more accurate estimations and as the grid size increases, our estimations get closer to the theoretical formula for MLE and MAP.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
